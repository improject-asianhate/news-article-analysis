{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Analysis\n",
    "\n",
    "## Main Research Questions\n",
    "2021 年 3 月事件 (Atlanta Spa shootings) 發生前後的比較：\n",
    "- 排名前十個概念 (concept) 是否有顯著不同\n",
    "- 報導 Anti-Asian Racism 態度上是否不同（中立性、支持 Asian 性、強調 Anti-Asian 對社會上種族不平等的嚴重性）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>202004</td>\n",
       "      <td>\\nAt China King last month, the phone rarely r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202004</td>\n",
       "      <td>\\nOn Wednesday morning, a Newton South High Sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>202004</td>\n",
       "      <td>\\nThe coronavirus pandemic has unleashed an ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>202004</td>\n",
       "      <td>\\nA year ago, Lunar New Year celebrations unfo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>202004</td>\n",
       "      <td>\\nIt happened in Chinatown first.\\nBefore we h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     date                                           contents\n",
       "0  202004  \\nAt China King last month, the phone rarely r...\n",
       "1  202004  \\nOn Wednesday morning, a Newton South High Sc...\n",
       "2  202004  \\nThe coronavirus pandemic has unleashed an ex...\n",
       "3  202004  \\nA year ago, Lunar New Year celebrations unfo...\n",
       "4  202004  \\nIt happened in Chinatown first.\\nBefore we h..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read from date.txt and store the contents into a dictionary\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "df_news = pd.read_csv('news.csv')\n",
    "df_news.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    elif treebank_tag.startswith('S'):\n",
    "        return wordnet.ADJ_SAT\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def preprocess(articles):\n",
    "    tokenizer = RegexpTokenizer(r'\\b[^\\'\\d\\W]+\\b')\n",
    "    nltk_stopwords = stopwords.words('english')\n",
    "    our_stopwords = ['publication', 'document', 'newspaper', 'database', 'login', 'docview', 'copyright', 'se', 'llc', 'inc', 'id', 'title', 'citation', 'doi', 'mr']\n",
    "    # Lowercase\n",
    "    articles = [article.lower() for article in articles]\n",
    "    # Remove Chinese characters\n",
    "    articles = [re.sub(r'[\\u4e00-\\u9fff]+', '', article) for article in articles]\n",
    "    # Execute tokenization\n",
    "    tokenized_articles = [tokenizer.tokenize(article) for article in articles]\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    preprocessed_articles = [pos_tag(article) for article in tokenized_articles]\n",
    "    # Keep only nouns, verbs\n",
    "    preprocessed_articles = [[(word, pos) for word, pos in article if pos in ['NN', 'NNS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']] for article in preprocessed_articles]\n",
    "    preprocessed_articles = [[lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in article] for article in preprocessed_articles]\n",
    "    # Remove words with length less than 3\n",
    "    preprocessed_articles = [[word for word in article if len(word) > 2] for article in preprocessed_articles]\n",
    "    # Stopwords removal\n",
    "    preprocessed_articles = [[word for word in article if word.lower() not in nltk_stopwords] for article in preprocessed_articles]\n",
    "    preprocessed_articles = [[word for word in article if word.lower() not in our_stopwords] for article in preprocessed_articles]\n",
    "    return preprocessed_articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "df_news = df_news.dropna()\n",
    "df_news['date'] = pd.to_datetime(df_news['date'], format='%Y%m')\n",
    "\n",
    "df_news_before_202103 = df_news[df_news['date'] < '2021-03-01']\n",
    "df_news_after_202103 = df_news[df_news['date'] >= '2021-03-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal = 'after' # Choose 'before' or 'after'\n",
    "articles_to_preprocess = df_news_after_202103['contents'].tolist()\n",
    "preprocessed_articles = preprocess(articles_to_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date\n",
       "2020-03-01    17\n",
       "2020-04-01    40\n",
       "2020-06-01    21\n",
       "2020-07-01     1\n",
       "2020-09-01     3\n",
       "2020-10-01     7\n",
       "2021-01-01     2\n",
       "2021-02-01    30\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news_before_202103.groupby('date').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TF-IDF corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(preprocessed_articles)\n",
    "dictionary.filter_extremes(no_below=7, no_above=0.3)\n",
    "corpus = [dictionary.doc2bow(article) for article in preprocessed_articles]\n",
    "\n",
    "# Get TF-IDF scores\n",
    "tfidf = TfidfModel(corpus)  \n",
    "tfidf_corpus = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LDA model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用不同參數訓練 LDA ，並選擇一致性最高的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import pandas as pd\n",
    "\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=5, step=1):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        # Train LDA model\n",
    "        model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=100, update_every=1, chunksize=100, passes=10, alpha='auto')\n",
    "        model_list.append(model)\n",
    "        # Compute coherence value\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='u_mass')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute coherence values\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=preprocessed_articles, start=10, limit=15, step=1)\n",
    "best_model = model_list[coherence_values.index(max(coherence_values))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "輸出各主題的前 20 名關鍵字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of keywords to 20\n",
    "num_keywords = 20\n",
    "# Get the top keywords for each topic\n",
    "topics = best_model.show_topics(num_topics=-1, num_words=num_keywords, formatted=False)\n",
    "# Create a dictionary to store the keywords\n",
    "keywords = {}\n",
    "for topic in topics:\n",
    "    keywords[topic[0]] = [word for word, _ in topic[1]]\n",
    "\n",
    "# Save the keywords\n",
    "with open(f'keywords_{goal}.json', 'w') as f:\n",
    "    json.dump(keywords, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the dominant topic for each document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "對每篇文章選出最主要的主題。\n",
    "\n",
    "選擇方法：每篇文章都有不同主題的機率分配，我們於此選出機率最高的主題。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 0.7950139)\n"
     ]
    }
   ],
   "source": [
    "# Get the most probable topic for each document\n",
    "topics = best_model[corpus]\n",
    "topics = [max(topic, key=lambda x: x[1]) for topic in topics]\n",
    "print(topics[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the dominant documents for each topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "對於每個主題，選擇最具代表性的兩篇文章，並儲存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 10 best documents for each topic\n",
    "# Convert topics to a dataframe\n",
    "topics_df = pd.DataFrame(topics, columns=['dominant topic', 'prob'])\n",
    "topics_df['doc_id'] = topics_df.index\n",
    "# Select 10 best documents for each topic\n",
    "best_docs = topics_df.groupby('dominant topic').apply(lambda x: x.nlargest(2, 'prob')).reset_index(drop=True)\n",
    "# Add articles to topics_df\n",
    "best_docs['contents'] = [articles_to_preprocess[doc_id] for doc_id in best_docs['doc_id']]\n",
    "# Save the best documents\n",
    "best_docs.to_csv(f'best_docs_{goal}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/dtypes/cast.py:1429: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  return np.find_common_type(types, [])\n"
     ]
    }
   ],
   "source": [
    "import pyLDAvis.gensim_models\n",
    "import pyLDAvis\n",
    "\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(best_model, corpus, dictionary)\n",
    "pyLDAvis.save_html(vis, f'lda_{goal}_202103.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('april', 'NN')]\n",
      "[('tuesday', 'NN')]\n",
      "[('wednesday', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Check the pos tag for \"april, tuesday, wednesday, thursday, friday, saturday, sunday\"\n",
    "\n",
    "print(pos_tag(['april']))\n",
    "print(pos_tag(['tuesday']))\n",
    "print(pos_tag(['wednesday']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use OpenAI API to interpret the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
