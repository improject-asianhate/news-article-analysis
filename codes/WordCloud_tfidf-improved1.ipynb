{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f175294e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "all_files = os.listdir(\"txt\")\n",
    "data = pd.DataFrame(columns=['id','content'])\n",
    "\n",
    "for file in all_files:\n",
    "    with open(\"txt/\" + file, 'r', encoding='utf-8') as f:\n",
    "        data.loc[len(data.index)] = [int(file.replace('.txt', '')), f.readlines()]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30a50a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[7/1/2021\\n, https://www-proquest-com.falcon.l...</td>\n",
       "      <td>Boston Globe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[7/2/2021\\n, https://www-proquest-com.falcon.l...</td>\n",
       "      <td>Boston Globe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>[Find a copy\\n, Abstract\\n, document 1 of 1\\n,...</td>\n",
       "      <td>Boston Globe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>[6/25/2021\\n, https://www-proquest-com.falcon....</td>\n",
       "      <td>Others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101</td>\n",
       "      <td>[6/25/2021\\n, https://www-proquest-com.falcon....</td>\n",
       "      <td>Others</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                            content        source\n",
       "0    0  [7/1/2021\\n, https://www-proquest-com.falcon.l...  Boston Globe\n",
       "1    1  [7/2/2021\\n, https://www-proquest-com.falcon.l...  Boston Globe\n",
       "2   10  [Find a copy\\n, Abstract\\n, document 1 of 1\\n,...  Boston Globe\n",
       "3  100  [6/25/2021\\n, https://www-proquest-com.falcon....        Others\n",
       "4  101  [6/25/2021\\n, https://www-proquest-com.falcon....        Others"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_path = 'type.txt'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    sources = json.load(file)\n",
    "\n",
    "id_source = pd.DataFrame.from_dict(sources, orient='index').T.melt(var_name='source', value_name='id').dropna(subset=['id'])\n",
    "data = pd.merge(data, id_source, on='id')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02104bbf",
   "metadata": {},
   "source": [
    "# STEP 1 : Data Cleaning\n",
    "\n",
    "- get correct paragraph based on 'full text' and 'subject'\n",
    "- delete URL and Chinese\n",
    "- tokenizat\n",
    "- stopwords removal\n",
    "- pos tag -> lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c462b6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows with an empty list in 'content': 0\n",
      "Removed rows with an empty list in 'content'.\n"
     ]
    }
   ],
   "source": [
    "empty_list_count = len(data[data['content'].apply(len) == 0])\n",
    "\n",
    "print(\"The number of rows with an empty list in 'content':\", empty_list_count)\n",
    "data = data[data['content'].apply(len) > 0]\n",
    "\n",
    "print(\"Removed rows with an empty list in 'content'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "145f40d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "        \n",
    "def get_clean_sentences(row):\n",
    "    sentences = [sentence[:-1].lower() for sentence in row['content']]\n",
    "    if \"full text\" in sentences:\n",
    "        start_idx = sentences.index(\"full text\")\n",
    "        sentences = sentences[start_idx+1:]\n",
    "    if \"subject\" in sentences:\n",
    "        end_idx = sentences.index(\"subject\")\n",
    "        sentences = sentences[:end_idx]\n",
    "        \n",
    "    pattern_url = r'^https:\\/\\/'\n",
    "    pattern_chinese = re.compile(r'[\\u4e00-\\u9fff]+') \n",
    "    result = [sentence for sentence in sentences if not re.search(pattern_url, sentence) and not re.search(pattern_chinese, sentence)]\n",
    "    return result\n",
    "\n",
    "data['clean_sentences'] = data.apply(get_clean_sentences, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c3d725f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\henry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\henry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\henry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\henry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>source</th>\n",
       "      <th>clean_sentences</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[7/1/2021\\n, https://www-proquest-com.falcon.l...</td>\n",
       "      <td>Boston Globe</td>\n",
       "      <td>[at china king last month, the phone rarely ra...</td>\n",
       "      <td>[china, king, last, month, phone, rarely, rang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[7/2/2021\\n, https://www-proquest-com.falcon.l...</td>\n",
       "      <td>Boston Globe</td>\n",
       "      <td>[on wednesday morning, a newton south high sch...</td>\n",
       "      <td>[wednesday, morning, newton, south, high, scho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>[Find a copy\\n, Abstract\\n, document 1 of 1\\n,...</td>\n",
       "      <td>Boston Globe</td>\n",
       "      <td>[when she first read the headlines from georgi...</td>\n",
       "      <td>[first, read, headline, georgia, danielle, kim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>[6/25/2021\\n, https://www-proquest-com.falcon....</td>\n",
       "      <td>Others</td>\n",
       "      <td>[the fbi warned of an increase in hate crimes ...</td>\n",
       "      <td>[fbi, warn, increase, hate, crime, asian, amer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101</td>\n",
       "      <td>[6/25/2021\\n, https://www-proquest-com.falcon....</td>\n",
       "      <td>Others</td>\n",
       "      <td>[discrimination against china and chinese peop...</td>\n",
       "      <td>[discrimination, china, chinese, people, nothi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                            content        source  \\\n",
       "0    0  [7/1/2021\\n, https://www-proquest-com.falcon.l...  Boston Globe   \n",
       "1    1  [7/2/2021\\n, https://www-proquest-com.falcon.l...  Boston Globe   \n",
       "2   10  [Find a copy\\n, Abstract\\n, document 1 of 1\\n,...  Boston Globe   \n",
       "3  100  [6/25/2021\\n, https://www-proquest-com.falcon....        Others   \n",
       "4  101  [6/25/2021\\n, https://www-proquest-com.falcon....        Others   \n",
       "\n",
       "                                     clean_sentences  \\\n",
       "0  [at china king last month, the phone rarely ra...   \n",
       "1  [on wednesday morning, a newton south high sch...   \n",
       "2  [when she first read the headlines from georgi...   \n",
       "3  [the fbi warned of an increase in hate crimes ...   \n",
       "4  [discrimination against china and chinese peop...   \n",
       "\n",
       "                                               words  \n",
       "0  [china, king, last, month, phone, rarely, rang...  \n",
       "1  [wednesday, morning, newton, south, high, scho...  \n",
       "2  [first, read, headline, georgia, danielle, kim...  \n",
       "3  [fbi, warn, increase, hate, crime, asian, amer...  \n",
       "4  [discrimination, china, chinese, people, nothi...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        # As default pos in lemmatization is Noun\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "def lemmatize(tokens):\n",
    "    # find the pos tagginf for each tokens [('What', 'WP'), ('can', 'MD'), ('I', 'PRP') ....\n",
    "    pos_tokens = nltk.pos_tag(tokens)\n",
    "    \n",
    "    # lemmatization using pos tag\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    pos_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in pos_tokens]\n",
    "    return pos_tokens\n",
    "\n",
    "def remove_punctuation_only_strings(strings):\n",
    "    punctuation_pattern = re.compile(r'^[^\\w\\s]+$')\n",
    "    filtered_strings = [s for s in strings if not punctuation_pattern.match(s)]\n",
    "    return filtered_strings\n",
    "\n",
    "def tokenize(row):\n",
    "    sentences = ''.join(row['clean_sentences'])\n",
    "    words = word_tokenize(sentences)\n",
    "    \n",
    "    stops = stopwords.words('english')\n",
    "    stops.extend([\"'s\",\"n't\"])  \n",
    "    words = [word for word in words if not word in stops]\n",
    "    # words = [word for word in words if not word in set(stopwords.words('english'))]\n",
    "    \n",
    "    # words = [word for word in words if not all(char in string.punctuation for char in word)]\n",
    "    words = remove_punctuation_only_strings(words)\n",
    "    \n",
    "    words = [word for word in words if len(word) != 1]\n",
    "    \n",
    "    words = lemmatize(words)\n",
    "    return words\n",
    "\n",
    "\n",
    "data['words'] = data.apply(tokenize, axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c4c68e",
   "metadata": {},
   "source": [
    "# STEP 2 : TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eeab5dc-9cd5-40e4-a8c6-3d1c20c4e0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立詞彙列表\n",
    "word_list = list(set(word for document in data['words'] for word in document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83077639-02d6-4917-ae28-1e4640261bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算詞頻（TF）\n",
    "def calculate_tf(word, document):\n",
    "    return document.count(word) / len(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7f72db4-aca5-4b94-b5e3-c4081a783599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算文件頻率（DF）\n",
    "def calculate_df(word_matrix, word_list):\n",
    "    df_matrix = []\n",
    "    for document in word_matrix:\n",
    "        word_dic = {}\n",
    "        for word in word_list:\n",
    "            if word in document:\n",
    "                word_dic[word] = 1\n",
    "            else:\n",
    "                word_dic[word] = 0\n",
    "        df_matrix.append(word_dic)\n",
    "    return df_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4cc36b3-418b-4b6d-a87e-b7fac604ab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 計算逆文件頻率(IDF)\n",
    "def calculate_idf(df_matrix):\n",
    "    file_path = 'type.txt'\n",
    "    with open(file_path, 'r') as file:\n",
    "        sources = json.load(file)\n",
    "    idf_matrix = {}\n",
    "    for key, docs in sources.items():\n",
    "        print(key)\n",
    "        value = {}\n",
    "        for doc in docs:\n",
    "            for k, df in df_matrix[doc].items():\n",
    "                if k in value:\n",
    "                    value[k] = df + value[k]\n",
    "                else:\n",
    "                    value[k] = df\n",
    "        for k, df in value.items():\n",
    "            value[k] = 2 + df/max(1,(len(df_matrix)-df))\n",
    "        idf_matrix[key] = value \n",
    "    return idf_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db99d064-0bdd-4906-bdf1-e27fda03414e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matrix = calculate_df(data['words'], word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ecbe760-b278-4f42-8a03-d6fde5d8ffc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston Globe\n",
      "Chicago Tribune\n",
      "LA Times\n",
      "New York Times\n",
      "News Magazines\n",
      "Others\n",
      "Star Tribune\n",
      "USA Today\n",
      "Washington Post\n"
     ]
    }
   ],
   "source": [
    "idf_matrix = calculate_idf(df_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11da3b1",
   "metadata": {},
   "source": [
    "## loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24138077-a4dd-4fee-b3d3-c03b668d4174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston Globe\n",
      "['say', 'asian', 'people', 'coronavirus', 'china', 'hate', 'state', 'chinese', 'virus', 'report', 'american', 'new', 'covid-19', 'group', 'attack', 'community', 'back', 'racism', 'appear', 'go', 'pandemic', 'president', 'since', 'incident', 'many', 'come', 'trump', 'fbi', 'make', 'see', 'use', 'start', 'last', 'call', 'one', 'donald', 'country', 'first', 'anti-asian', \"'re\", 'u', 'take', 'victim', 'day', 'would', 'right', 'stop', 'march', 'director', 'racist']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(top50)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 创建一个 WordCloud 对象，并传入排序后的字典作为参数\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m wordcloud \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m800\u001b[39m, height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m, background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhite\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mgenerate_from_frequencies(sorted_idf)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# 绘制文字云\u001b[39;00m\n\u001b[0;32m     12\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "for key, idf in idf_matrix.items():\n",
    "    print(key)\n",
    "    # 按值对字典进行排序，并选择前 50 个\n",
    "    sorted_idf = dict(sorted(idf.items(), key=lambda item: item[1], reverse=True)[:50])\n",
    "    top50 = []\n",
    "    for k, value in sorted_idf.items():\n",
    "        top50.append(k)\n",
    "    print(top50)\n",
    "    # 创建一个 WordCloud 对象，并传入排序后的字典作为参数\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(sorted_idf)\n",
    "    # 绘制文字云\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152091c3-eb96-476b-9a8e-295c740fe76e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
