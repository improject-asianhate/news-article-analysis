{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load OpenAI's LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "### Load Glossary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126 documents loaded\n"
     ]
    }
   ],
   "source": [
    "# Load the pdf documents from ./glossary\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "loader = PyPDFDirectoryLoader(\"./glossary\")\n",
    "docs = loader.load()\n",
    "print(len(docs), \"documents loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split\n",
    "Our loaded document is over 42k characters long. This is too long to fit in the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\n",
    "\n",
    "To handle this weâ€™ll split the Document into chunks for embedding and vector storage. This should help us retrieve only the most relevant bits of the blog post at run time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store\n",
    "\n",
    "Now we need to index our text chunks so that we can search over them at runtime. The most common way to do this is to **embed the contents of each document split** and **insert these embeddings into a vector database** (or vector store)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma # Chroma vector store\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval and Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})  \n",
    "retrieved_docs = retriever.invoke(\"What is Model Minority?\")\n",
    "\n",
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the example articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "len_articles = 5\n",
    "example_articles = []\n",
    "for i in range(1, len_articles + 1):\n",
    "    with open(f\"./articles/{i}.txt\", \"r\") as f:\n",
    "        example_articles.append(f.read())\n",
    "\n",
    "print(len(example_articles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the concept tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./concept_tree.txt\", \"r\") as f:\n",
    "    concept_tree = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Citation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from operator import itemgetter\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class CitedAnswer(BaseModel):\n",
    "    \"\"\"Answer the user question based only on the given sources, and cite the sources used.\"\"\"\n",
    "\n",
    "    answer: str = Field(\n",
    "        ...,\n",
    "        description=\"The analysis the user's news article (include main topics and relevant concepts), which is based only on the given sources and the concept tree.\",\n",
    "    )\n",
    "    citations: List[int] = Field(\n",
    "        ...,\n",
    "        description=\"The source name of the SPECIFIC sources which justify the answer.\",\n",
    "    )\n",
    "\n",
    "structured_llm = llm.with_structured_output(CitedAnswer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "def format_docs_with_id(docs):\n",
    "    formatted = [\n",
    "        f\"Source ID: {i}\\nArticle Title: {doc.metadata['source']}\\nArticle Snippet: {doc.page_content}\"\n",
    "        for i, doc in enumerate(docs)\n",
    "    ]\n",
    "    return \"\\n\\n\" + \"\\n\\n\".join(formatted)\n",
    "\n",
    "template = \"\"\"\n",
    "You are a helpful assistant who can analyze news articles using provided context and a concept tree. Use the following context and concept tree to analyze the article at the end.\n",
    "\n",
    "Context: {context}\n",
    "Concept Tree: {concept_tree}\n",
    "Article ID: {article_id}\n",
    "Article: {input}\n",
    "\n",
    "Analyze the article and output your analysis in .csv format with the following columns:\n",
    "\n",
    "1. ARTICLE_ID: The unique identifier for the article (provided above)\n",
    "2. CHOSEN_CONCEPT: The exact name of a relevant concept from the tree\n",
    "3. EXPLANATION: Brief explanation of why the concept is relevant, including a quote if applicable\n",
    "\n",
    "When identifying relevant concepts from the tree, consider both explicit mentions and implicit references. Include parent concepts of your chosen concept in output, each concept should be seperated by /.\n",
    "\n",
    "Include both explicitly mentioned and implicitly referenced concepts. If there are ambiguities or multiple interpretations, include them in the EXPLANATION column. Create a new row for each relevant concept.\n",
    "\n",
    "Begin your output with the header row, followed by the data rows:\n",
    "\n",
    "ARTICLE_ID##CHOSEN_CONCEPT##EXPLANATION\n",
    "\n",
    "Ensure that your output is strictly in ##-delimited format.\n",
    "\n",
    "Now, begin your ##-delimited output:\n",
    "\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs_with_id(x[\"context\"])), concept_tree= lambda x: concept_tree)\n",
    "    | custom_rag_prompt\n",
    "    | structured_llm\n",
    ")\n",
    "\n",
    "retrieve_docs = (lambda x: x[\"input\"]) | retriever\n",
    "\n",
    "chain = RunnablePassthrough.assign(context=retrieve_docs).assign(\n",
    "    answer=rag_chain_from_docs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i, example_article in enumerate(example_articles):\n",
    "    result = chain.invoke({\"input\": example_article, \"article_id\": i + 1})\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ss/9b82800n1bq54s_0p7_m551c0000gn/T/ipykernel_90470/3573805216.py:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(data, index_col=False, sep=\"##\")\n",
      "/var/folders/ss/9b82800n1bq54s_0p7_m551c0000gn/T/ipykernel_90470/3573805216.py:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(data, index_col=False, sep=\"##\")\n",
      "/var/folders/ss/9b82800n1bq54s_0p7_m551c0000gn/T/ipykernel_90470/3573805216.py:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(data, index_col=False, sep=\"##\")\n",
      "/var/folders/ss/9b82800n1bq54s_0p7_m551c0000gn/T/ipykernel_90470/3573805216.py:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(data, index_col=False, sep=\"##\")\n",
      "/var/folders/ss/9b82800n1bq54s_0p7_m551c0000gn/T/ipykernel_90470/3573805216.py:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(data, index_col=False, sep=\"##\")\n"
     ]
    }
   ],
   "source": [
    "df_all = pd.DataFrame()\n",
    "for result in results:\n",
    "    result_dict = dict(result[\"answer\"])\n",
    "    data = io.StringIO(result_dict['answer'])\n",
    "    df = pd.read_csv(data, index_col=False, sep=\"##\")\n",
    "    df_all = pd.concat([df_all, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv(\"output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset for evaluation\n",
    "# Columns: ARTICLE, CONCEPT_TREE, CHOSEN_CONCEPT, EXPLANATION\n",
    "df_eval = pd.DataFrame()\n",
    "df_eval = df_all.copy()\n",
    "df_eval[\"ARTICLE\"] = df_eval[\"ARTICLE_ID\"].apply(lambda x: example_articles[int(x)-1])\n",
    "df_eval[\"CONCEPT_TREE\"] = concept_tree\n",
    "df_eval = df_eval[[\"ARTICLE\", \"CONCEPT_TREE\", \"CHOSEN_CONCEPT\", \"EXPLANATION\"]]\n",
    "df_eval.to_csv(\"evaluation.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
