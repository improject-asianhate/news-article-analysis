{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Analysis\n",
    "\n",
    "## Main Research Questions\n",
    "2021 年 3 月事件 (Atlanta Spa shootings) 發生前後的比較：\n",
    "- 排名前十個概念 (concept) 是否有顯著不同\n",
    "- 報導 Anti-Asian Racism 態度上是否不同（中立性、支持 Asian 性、強調 Anti-Asian 對社會上種族不平等的嚴重性）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>contents</th>\n",
       "      <th>source</th>\n",
       "      <th>date</th>\n",
       "      <th>preprocessed_articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7/1/2021\\n https://www-proquest-com.falcon.lib...</td>\n",
       "      <td>Boston Globe</td>\n",
       "      <td>202004</td>\n",
       "      <td>['see', 'chinatown', 'restaurateur', 'face', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7/2/2021\\n https://www-proquest-com.falcon.lib...</td>\n",
       "      <td>Boston Globe</td>\n",
       "      <td>202004</td>\n",
       "      <td>['racist', 'zoom', 'bombing', 'invades', 'clas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>Find a copy\\n Abstract\\n document 1 of 1\\n Ful...</td>\n",
       "      <td>Boston Globe</td>\n",
       "      <td>202103</td>\n",
       "      <td>['asian_american', 'attack', 'hit', 'struggle'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>6/24/2021\\n https://www-proquest-com.falcon.li...</td>\n",
       "      <td>Others</td>\n",
       "      <td>202003</td>\n",
       "      <td>['world', 'human', 'right', 'dimension', 'covi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101</td>\n",
       "      <td>6/24/2021\\n https://www-proquest-com.falcon.li...</td>\n",
       "      <td>Others</td>\n",
       "      <td>202003</td>\n",
       "      <td>['trump', 'brush', 'remark', 'claim', 'asian_a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                           contents        source  \\\n",
       "0    0  7/1/2021\\n https://www-proquest-com.falcon.lib...  Boston Globe   \n",
       "1    1  7/2/2021\\n https://www-proquest-com.falcon.lib...  Boston Globe   \n",
       "2   10  Find a copy\\n Abstract\\n document 1 of 1\\n Ful...  Boston Globe   \n",
       "3  100  6/24/2021\\n https://www-proquest-com.falcon.li...        Others   \n",
       "4  101  6/24/2021\\n https://www-proquest-com.falcon.li...        Others   \n",
       "\n",
       "     date                              preprocessed_articles  \n",
       "0  202004  ['see', 'chinatown', 'restaurateur', 'face', '...  \n",
       "1  202004  ['racist', 'zoom', 'bombing', 'invades', 'clas...  \n",
       "2  202103  ['asian_american', 'attack', 'hit', 'struggle'...  \n",
       "3  202003  ['world', 'human', 'right', 'dimension', 'covi...  \n",
       "4  202003  ['trump', 'brush', 'remark', 'claim', 'asian_a...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read from date.txt and store the contents into a dictionary\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "df_news = pd.read_csv('../results/preprocessed_news.csv')\n",
    "preprocessed_articles = [article.strip(\"['\").strip(\"']\").split(\"', '\") for article in df_news['preprocessed_articles'].tolist()]\n",
    "df_news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TF-IDF corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(preprocessed_articles)\n",
    "dictionary.filter_extremes(no_below=7, no_above=0.3)\n",
    "corpus = [dictionary.doc2bow(article) for article in preprocessed_articles]\n",
    "\n",
    "# Get TF-IDF scores\n",
    "tfidf = TfidfModel(corpus)  \n",
    "tfidf_corpus = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LDA model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用不同參數訓練 LDA ，並選擇一致性最高的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import pandas as pd\n",
    "\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=5, step=1):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        # Train LDA model\n",
    "        model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=100, update_every=1, chunksize=100, passes=10, alpha='auto')\n",
    "        model_list.append(model)\n",
    "        # Compute coherence value\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='u_mass')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute coherence values\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=preprocessed_articles, start=10, limit=15, step=1)\n",
    "best_model = model_list[coherence_values.index(max(coherence_values))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "輸出各主題的前 20 名關鍵字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of keywords to 20\n",
    "num_keywords = 20\n",
    "# Get the top keywords for each topic\n",
    "topics = best_model.show_topics(num_topics=-1, num_words=num_keywords, formatted=False)\n",
    "# Create a dictionary to store the keywords\n",
    "keywords = {}\n",
    "for topic in topics:\n",
    "    keywords[topic[0]] = [word for word, _ in topic[1]]\n",
    "\n",
    "# Save the keywords\n",
    "with open(f'../results/keywords_time.json', 'w') as f:\n",
    "    json.dump(keywords, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the dominant topic for each document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "對每篇文章選出最主要的主題。\n",
    "\n",
    "選擇方法：每篇文章都有不同主題的機率分配，我們於此選出機率最高的主題。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most probable topic for each document\n",
    "topics = best_model[corpus]\n",
    "d = []\n",
    "id = 0\n",
    "for topic in topics:\n",
    "    o = {\"id\": df_news.iloc[id].id, \"odd\": topic}\n",
    "    d.append(o)\n",
    "    id += 1\n",
    "df = pd.DataFrame(d)\n",
    "df.to_csv(f\"../results/odd_media.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_category(odd_list):\n",
    "    max_category = None\n",
    "    max_prob = -1\n",
    "    for item in odd_list:\n",
    "        if item[1] > max_prob:\n",
    "            max_prob = item[1]\n",
    "            max_category = item[0]\n",
    "    return max_category\n",
    "\n",
    "# 將最大分類添加到新的 class 列中\n",
    "df['class'] = df['odd'].apply(find_max_category)\n",
    "class_counts = df['class'].value_counts()\n",
    "class_7_count = class_counts.get(7, 0)  # 如果沒有值為 7 的話，返回 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['date'] = pd.to_datetime(df_news['date'], format='%Y%m')\n",
    "\n",
    "df_news_before_202103 = df_news[df_news['date'] < '2021-03-01']\n",
    "df_news_after_202103 = df_news[df_news['date'] >= '2021-03-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_news_before_202103 =pd.merge(df, df_news_before_202103, on='id', suffixes=('_df1', '_df2'), how='inner')\n",
    "class_news_before_202103_counts = class_news_before_202103['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_news_after_202103 =pd.merge(df, df_news_after_202103, on='id', suffixes=('_df1', '_df2'), how='inner')\n",
    "class_news_after_202103_counts = class_news_after_202103['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list = df['class'].unique().tolist()\n",
    "r_news_before_202103 = []\n",
    "r_news_after_202103 = []\n",
    "for c in class_list:\n",
    "    r_news_before_202103.append((c, class_news_before_202103_counts.get(c,0)/class_counts.get(c, 1)/df_news_before_202103.shape[0]))\n",
    "    r_news_after_202103.append((c, class_news_after_202103_counts.get(c,0)/class_counts.get(c, 1)/df_news_after_202103.shape[0]))\n",
    "r_news_before_202103 = sorted(r_news_before_202103, key=lambda x: x[0])\n",
    "r_news_after_202103 = sorted(r_news_after_202103, key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "x1, y1 = zip(*r_news_before_202103)\n",
    "x2, y2 = zip(*r_news_after_202103)\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.bar(np.array(x1) - 0.1, y1, width=0.2, color='blue', alpha=0.5, label='before_202103')\n",
    "ax.bar(np.array(x2) + 0.1, y2, width=0.2, color='red', alpha=0.5, label='after_202103')\n",
    "ax.set_title('Bar Chart')\n",
    "ax.set_xlabel('Class')\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_xticks(np.arange(len(x1)))\n",
    "ax.legend()\n",
    "plt.savefig('../results/bar_chart_time.png', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the dominant documents for each topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "對於每個主題，選擇最具代表性的兩篇文章，並儲存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [max(topic, key=lambda x: x[1]) for topic in topics]\n",
    "# Select 10 best documents for each topic\n",
    "# Convert topics to a dataframe\n",
    "topics_df = pd.DataFrame(topics, columns=['dominant topic', 'prob'])\n",
    "topics_df['doc_id'] = topics_df.index\n",
    "# Select 10 best documents for each topic\n",
    "best_docs = topics_df.groupby('dominant topic').apply(lambda x: x.nlargest(2, 'prob')).reset_index(drop=True)\n",
    "# Save the best documents\n",
    "best_docs.to_csv(f'../results/best_docs_time.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
