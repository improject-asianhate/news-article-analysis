{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ea655ec-0ed0-456f-9ed3-497405c3c1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/yutingchen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/yutingchen/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fefc68-fc47-42b1-965b-ea59e2e19c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import base64\n",
    "from IPython import display\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.messages import HumanMessage, SystemMessage\n",
    "from langchain.schema.document import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from openai import OpenAIError\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = \"sk-****\"\n",
    "output_path = \"./images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08ea7e0c-29c5-489e-98b4-1654d04ac7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# 指定目錄路徑\n",
    "directory = 'Glossary of racism concepts'\n",
    "\n",
    "# 獲取該目錄下所有 PDF 檔案的清單\n",
    "pdf_files = [f for f in os.listdir(directory) if f.endswith('.pdf')]\n",
    "\n",
    "raw_pdf_elements = []\n",
    "\n",
    "# 迭代每個 PDF 檔案並讀取內容\n",
    "for pdf_file in pdf_files:\n",
    "    pdf_path = os.path.join(directory, pdf_file)\n",
    "    reader = PdfReader(pdf_path)\n",
    "    \n",
    "    # 將每頁的內容讀取到 raw_pdf_elements 中\n",
    "    for page_num in range(len(reader.pages)):\n",
    "        page = reader.pages[page_num]\n",
    "        raw_pdf_elements.append(page.extract_text())\n",
    "\n",
    "#print(raw_pdf_elements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4799f0f0-9684-4a31-b939-97ee3d9ce3e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 假設 summary_chain 是你用來生成摘要的函數\n",
    "text_elements = []\n",
    "text_summaries = []\n",
    "table_summaries = []\n",
    "table_elements = []\n",
    "\n",
    "summary_prompt = \"\"\"\n",
    "Summarize the following {element_type}: \n",
    "{element}\n",
    "\"\"\"\n",
    "summary_chain = LLMChain(\n",
    "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", max_tokens=1024), \n",
    "    prompt=PromptTemplate.from_template(summary_prompt)\n",
    ")\n",
    "\n",
    "for e in raw_pdf_elements:\n",
    "    if isinstance(e, str):  # 檢查是否為文本元素\n",
    "        text_elements.append(e)\n",
    "        summary = summary_chain.run({'element_type': 'text', 'element': e})\n",
    "        text_summaries.append(summary)\n",
    "    elif 'Table' in repr(e):  # 檢查是否為表格元素\n",
    "        table_elements.append(e)\n",
    "        summary = summary_chain.run({'element_type': 'table', 'element': e})\n",
    "        table_summaries.append(summary)\n",
    "\n",
    "# 檢查讀取到的內容\n",
    "\n",
    "#print(text_summaries)\n",
    "#print(table_summaries)\n",
    "#print(table_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2bd28c3-01b4-4cf2-ba20-3faddaa8f546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get image summaries\n",
    "image_elements = []\n",
    "image_summaries = []\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        return base64.b64encode(f.read()).decode('utf-8')\n",
    "    \n",
    "def summarize_image(encoded_image):\n",
    "    prompt = [\n",
    "        SystemMessage(content=\"You are a bot that is good at analyzing images.\"),\n",
    "        HumanMessage(content=[\n",
    "            {\n",
    "                \"type\": \"text\", \n",
    "                \"text\": \"Describe the contents of this image.\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "                },\n",
    "            },\n",
    "        ])\n",
    "    ]\n",
    "    response = ChatOpenAI(model=\"gpt-4-turbo\", max_tokens=1024).invoke(prompt)\n",
    "    return response.content\n",
    "    \n",
    "for i in os.listdir(output_path):\n",
    "    if i.endswith(('.png', '.jpg', '.jpeg')):\n",
    "        image_path = os.path.join(output_path, i)\n",
    "        encoded_image = encode_image(image_path)\n",
    "        image_elements.append(encoded_image)\n",
    "        summary = summarize_image(encoded_image)\n",
    "        image_summaries.append(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19facf31-47e2-44d5-bfb0-9f1ae9371e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Li Cheng Shuan\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Create Documents and Vectorstore\n",
    "documents = []\n",
    "retrieve_contents = []\n",
    "\n",
    "for e, s in zip(text_elements, text_summaries):\n",
    "    i = str(uuid.uuid4())\n",
    "    doc = Document(\n",
    "        page_content = s,\n",
    "        metadata = {\n",
    "            'id': i,\n",
    "            'type': 'text',\n",
    "            'original_content': e\n",
    "        }\n",
    "    )\n",
    "    retrieve_contents.append((i, e))\n",
    "    documents.append(doc)\n",
    "    \n",
    "for e, s in zip(table_elements, table_summaries):\n",
    "    doc = Document(\n",
    "        page_content = s,\n",
    "        metadata = {\n",
    "            'id': i,\n",
    "            'type': 'table',\n",
    "            'original_content': e\n",
    "        }\n",
    "    )\n",
    "    retrieve_contents.append((i, e))\n",
    "    documents.append(doc)\n",
    "    \n",
    "for e, s in zip(image_elements, image_summaries):\n",
    "    doc = Document(\n",
    "        page_content = s,\n",
    "        metadata = {\n",
    "            'id': i,\n",
    "            'type': 'image',\n",
    "            'original_content': e\n",
    "        }\n",
    "    )\n",
    "    retrieve_contents.append((i, s))\n",
    "    documents.append(doc)\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents=documents, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fabf3582-ab61-4c4e-8012-29008db0f4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_template = \"\"\"\n",
    "Answer the question based only on the following context, which can include text, images and tables:\n",
    "{context}\n",
    "Question: {question} \n",
    "\"\"\"\n",
    "answer_chain = LLMChain(llm=ChatOpenAI(model=\"gpt-3.5-turbo\", max_tokens=1024), prompt=PromptTemplate.from_template(answer_template))\n",
    "\n",
    "def answer(question):\n",
    "    relevant_docs = vectorstore.similarity_search(question)\n",
    "    context = \"\"\n",
    "    relevant_images = []\n",
    "    for d in relevant_docs:\n",
    "        if d.metadata['type'] == 'text':\n",
    "            context += '[text]' + d.metadata['original_content']\n",
    "        elif d.metadata['type'] == 'table':\n",
    "            context += '[table]' + d.metadata['original_content']\n",
    "        elif d.metadata['type'] == 'image':\n",
    "            context += '[image]' + d.page_content\n",
    "            relevant_images.append(d.metadata['original_content'])\n",
    "    result = answer_chain.run({'context': context, 'question': question})\n",
    "    return result, relevant_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "026d83eb-2f83-47fd-831b-d3534285f9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deviant is a term used to characterize Asian Americans as immoral and dangerous in one instance and meek and robotic in another. It is one of the subcategories used in the Committee of 100's Glossary of Anti-Asian Terms and Tropes.\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "from IPython import display\n",
    "result, relevant_images = answer(\"What is Deviant?\")\n",
    "print(result)\n",
    "for e in relevant_images:\n",
    "    display.display(display.Image(base64.b64decode(e)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c69147d-8820-4cde-96e3-85d18a145e98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
